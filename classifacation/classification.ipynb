{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cafe7f06",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-03T13:24:50.051185Z",
     "iopub.status.busy": "2025-12-03T13:24:50.050775Z",
     "iopub.status.idle": "2025-12-03T13:24:56.249073Z",
     "shell.execute_reply": "2025-12-03T13:24:56.248251Z"
    },
    "papermill": {
     "duration": 6.202729,
     "end_time": "2025-12-03T13:24:56.250648",
     "exception": false,
     "start_time": "2025-12-03T13:24:50.047919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DebertaV2Tokenizer\n",
    "\n",
    "class SimpleWSDDataset(Dataset):\n",
    "    \"\"\"最简单的WSD数据集\"\"\"\n",
    "    \n",
    "    def __init__(self, json_path, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.samples = []\n",
    "        \n",
    "        # 1. 读取JSON\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # 2. 展平：每个choice变成一个样本\n",
    "        for key, item in data.items():\n",
    "            homonym = item[\"homonym\"]\n",
    "            definition = item[\"judged_meaning\"]\n",
    "            example = item[\"example_sentence\"]\n",
    "            \n",
    "            # 完整上下文\n",
    "            context = f\"{item['precontext']} {item['sentence']} {item['ending']}\"\n",
    "            \n",
    "            # 为每个有效的choice创建一个样本\n",
    "            for choice_idx, (score, nonsensical) in enumerate(zip(item[\"choices\"], item[\"nonsensical\"])):\n",
    "                if not nonsensical:  # 只取有效的\n",
    "                    self.samples.append({\n",
    "                        \"homonym\": homonym,\n",
    "                        \"definition\": definition,\n",
    "                        \"example\": example,\n",
    "                        \"context\": context,\n",
    "                        \"score\": score,  # 1-5分\n",
    "                        \"sample_id\": f\"{item['sample_id']}_{choice_idx}\"  # 唯一ID\n",
    "                    })\n",
    "        \n",
    "        print(f\"创建了 {len(self.samples)} 个训练样本\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # 最简单的输入格式：定义 + 例句 + 待判断文本\n",
    "        # text = (\n",
    "        #     f\"homonym：{sample['homonym']} [SEP] \"\n",
    "        #     f\"Definition:{sample['definition']} [SEP] \"\n",
    "        #     f\"Example:{sample['example']} [SEP] \"\n",
    "        #     f\"Context:{sample['context']}\"\n",
    "        # )\n",
    "        \n",
    "        text_parts = (\n",
    "            f\"homonym：{sample['homonym']}\"\n",
    "            f\"Definition:{sample['definition']}\"\n",
    "            f\"Example:{sample['example']}\"\n",
    "            f\"Context:{sample['context']}\"\n",
    "        )\n",
    "        text = self.tokenizer.sep_token.join(text_parts)\n",
    "        \n",
    "        # Tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # 移除batch维度\n",
    "        encoding = {k: v.squeeze(0) for k, v in encoding.items()}\n",
    "        \n",
    "        \n",
    "        \n",
    "        # 添加labels\n",
    "        # encoding[\"labels\"] = torch.tensor(sample[\"score\"], dtype=torch.float32)\n",
    "        # 回归的时候再用这个\n",
    "        \n",
    "        \n",
    "        score_index = sample[\"score\"] - 1  # 转为0-4索引\n",
    "        encoding[\"labels\"] = torch.tensor(score_index, dtype=torch.long)\n",
    "        \n",
    "        if \"token_type_ids\" in encoding:\n",
    "             del encoding[\"token_type_ids\"]\n",
    "             \n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "692ddd7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T13:24:56.255061Z",
     "iopub.status.busy": "2025-12-03T13:24:56.254603Z",
     "iopub.status.idle": "2025-12-03T13:25:25.940259Z",
     "shell.execute_reply": "2025-12-03T13:25:25.939610Z"
    },
    "papermill": {
     "duration": 29.689153,
     "end_time": "2025-12-03T13:25:25.941621",
     "exception": false,
     "start_time": "2025-12-03T13:24:56.252468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-03 13:25:05.258476: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764768305.492037      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764768305.565587      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch.nn as nn\n",
    "from transformers import DebertaV2Model, DebertaV2PreTrainedModel\n",
    "\n",
    "# 类别数量是 5 （对应 1, 2, 3, 4, 5 分）\n",
    "NUM_LABELS = 5 \n",
    "\n",
    "class DebertaV2ForWSDScoring(DebertaV2PreTrainedModel):\n",
    "    \"\"\"\n",
    "    继承 DeBERTaV2Model，添加一个输出5个类别的分类头，\n",
    "    用于 1-5 分的合理性评分任务。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "        # 1. DeBERTaV2 主体 (用于特征提取)\n",
    "        self.deberta = DebertaV2Model(config)\n",
    "        \n",
    "        # 2. 分类头 (Classification Head)\n",
    "        self.classifier = nn.Sequential(\n",
    "            # dropout rate从config中获取\n",
    "            nn.Dropout(config.hidden_dropout_prob), \n",
    "            # 将 DeBERTaV2 的隐藏状态维度映射到 5 个类别\n",
    "            nn.Linear(config.hidden_size, NUM_LABELS) \n",
    "        )\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.post_init() \n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
    "        \n",
    "        # 运行 DeBERTaV2 主体\n",
    "        # DeBERTaV2 模型的 forward 默认不使用 token_type_ids，如果数据集中有，会被 **kwargs 吸收\n",
    "        outputs = self.deberta(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "        )\n",
    "        \n",
    "        # 提取 [CLS] token 的隐藏状态（第一个 token 的输出）\n",
    "        # (batch_size, hidden_size)\n",
    "        cls_output = outputs[0][:, 0, :]\n",
    "        \n",
    "        # 运行分类头\n",
    "        # (batch_size, NUM_LABELS)\n",
    "        logits = self.classifier(cls_output) \n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # 交叉熵损失函数\n",
    "            # logits: (batch_size, 5)\n",
    "            # labels: (batch_size,) 且值范围在 [0, 4]\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, NUM_LABELS), labels.view(-1))\n",
    "\n",
    "        # 返回结果 (loss, logits) 或 logits\n",
    "        return (loss, logits) if loss is not None else logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f8b0d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-03T13:25:25.946395Z",
     "iopub.status.busy": "2025-12-03T13:25:25.945888Z",
     "iopub.status.idle": "2025-12-03T16:14:49.913918Z",
     "shell.execute_reply": "2025-12-03T16:14:49.913061Z"
    },
    "papermill": {
     "duration": 10163.972818,
     "end_time": "2025-12-03T16:14:49.916191",
     "exception": false,
     "start_time": "2025-12-03T13:25:25.943373",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForWSDScoring were not initialized from the model checkpoint at /kaggle/input/semeval/deberta-v3-large and are newly initialized: ['classifier.1.bias', 'classifier.1.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_19/1651077949.py:56: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "创建了 11166 个训练样本\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始微调 DeBERTaV2 模型...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4188' max='4188' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4188/4188 2:49:03, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>6.723600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>6.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>6.571500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>6.506800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>6.589500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>6.523100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>6.261800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>6.536400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>6.568400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.499100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>6.345100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>6.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>6.398100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>6.313300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>6.489200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>6.560800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>6.512700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>6.389600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>6.404800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>6.514400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>6.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>6.429200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>6.443200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>6.435500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>6.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>6.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>6.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>6.337300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>6.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>6.423600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>6.318700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>6.494700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>6.369700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>6.414900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>6.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>6.363600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>6.416000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>6.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>6.374300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>6.331600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>6.466400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>6.444600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>6.507600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>6.358300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>6.500900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>6.422900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>6.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>6.356800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>6.374600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>6.452900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>6.311400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>6.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>6.410200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>6.491400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>6.407800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>6.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>6.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>6.399700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>6.462700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>6.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>6.440100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>6.382500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>6.375300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>6.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>6.376600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>6.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>6.529400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>6.265300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>6.435800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>6.300900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>6.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>6.422600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>6.463400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>6.365500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>6.547900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>6.444000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>6.449900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>6.478700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>6.425200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>6.270400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>6.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>6.258300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>6.449800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/kaggle/working/tokenizer_config.json',\n",
       " '/kaggle/working/special_tokens_map.json',\n",
       " '/kaggle/working/spm.model',\n",
       " '/kaggle/working/added_tokens.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2Config, Trainer, TrainingArguments\n",
    "\n",
    "# from model import DebertaV2ForWSDScoring\n",
    "# from data_load import SimpleWSDDataset\n",
    "\n",
    "\n",
    "# 建议使用相对较小的版本开始，以节省资源\n",
    "MODEL_NAME = \"/kaggle/input/semeval/deberta-v3-large\" \n",
    "\n",
    "# 1. 加载 Tokenizer\n",
    "tokenizer = DebertaV2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# 2. 加载配置（用于初始化您的模型类）\n",
    "config = DebertaV2Config.from_pretrained(MODEL_NAME)\n",
    "config.num_labels = 5 # 确保配置中标签数正确\n",
    "\n",
    "# 3. 初始化您的自定义模型\n",
    "# 假设您已在脚本中导入了 DebertaV2ForWSDScoring 类\n",
    "model = DebertaV2ForWSDScoring.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    config=config\n",
    ")\n",
    "\n",
    "# 导入 SimpleWSDDataset 类\n",
    "train_dataset = SimpleWSDDataset(\n",
    "    json_path=\"/kaggle/input/semeval/data/train.json\", \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = \"/kaggle/working/\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,                     \n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    # gradient_checkpointing=True,\n",
    "    # ------------------------------------------------\n",
    "    # 核心修改：禁用评估\n",
    "    eval_strategy=\"no\",               # <--- 禁用评估\n",
    "    load_best_model_at_end=False,           # <--- 禁用加载最佳模型\n",
    "    # ------------------------------------------------\n",
    "    warmup_steps=500,                       \n",
    "    weight_decay=0.01,                      \n",
    "    logging_dir='./logs',                   \n",
    "    logging_steps=50,                       \n",
    "    save_strategy=\"epoch\",                  # 训练完每一轮就保存一次\n",
    "    learning_rate=2e-5,                     \n",
    "    fp16=True,                              \n",
    ")\n",
    "\n",
    "# 实例化 Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 启动训练\n",
    "print(\"开始微调 DeBERTaV2 模型...\")\n",
    "trainer.train()\n",
    "\n",
    "# 训练结束后，保存最终模型\n",
    "trainer.save_model(OUTPUT_DIR)\n",
    "tokenizer.save_pretrained(OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8903723,
     "sourceId": 13971926,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10206.851856,
   "end_time": "2025-12-03T16:14:53.224770",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-03T13:24:46.372914",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
